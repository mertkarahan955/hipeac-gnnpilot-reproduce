\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pifont} % for checkmark and xmark symbols
\lstset{
  basicstyle=\ttfamily\footnotesize,
  breaklines=true,
  breakatwhitespace=true,
  columns=fullflexible,
  showstringspaces=false,
  backgroundcolor=\color{white},
  frame=single,
  upquote=true
}

\begin{document}

\title{Reproducing GNNPilot: A Holistic Framework for High-Performance Graph Neural Network Computations on GPUs}

\graphicspath{{multi_dataset_results/plots/}}

\author{
Kübra Holt\\Izmir Institute of Technology\\Turkey\and
Advisor: Işıl Öz,\\Izmir Institute of Technology\\Turkey\and
Mert Karahan\\Izmir Institute of Technology\\Turkey
}

\maketitle

\begin{abstract}
This work presents a relatively small reproduction compare to \textit{GNNPilot}, a holistic GPU optimization framework for accelerating Graph Neural Networks (GNNs). As part of the HiPEAC 2026 Student Challenge (Option B), we rebuilt and evaluated the open-source codebase of GNNPilot, focusing on its gather-operator optimizations, dynamic parallelization strategies, kernel fusion mechanisms, and sampling-based auto-tuning system. Our reproduction was conducted under constrained hardware conditions (NVIDIA GTX~1060 6GB) and required quite a bit effort to resolve missing dependencies, undocumented build steps, and multi-paper code interactions within the original repository. Despite these challenges, we solved the dependency conflicts and pass beyond the hardware limitations with workarounds and also most important was contacted with the authors and received the code parts that were missing firstly,  so that we successfully executed all major kernel variants and end-to-end GAT tests.

Our results confirm some of the core claims of the original paper: neighbor packing significantly improves load balancing for sparse graphs, bin packing benefits higher-degree graphs through improved locality, and dynamic parallelization combined with lightweight auto-tuning reliably selects the best-performing kernel among 32 strategy variants. Across 9 diverse datasets ranging from 105 nodes (polbooks) to 192K nodes (caidaRouterLevel), we observe execution times from 0.050~ms to 3.55~ms for best kernels, with auto-tuning achieving up to 195$\times$ speedup between optimal and worst-case kernel selections. Overall, our findings validate GNNPilot's design principles and demonstrate that its optimizations remain effective even on relatively old hardware and scale across diverse graph structures.
\end{abstract}


\section{Goal}
Our primary goal in this study is to reproduce the experimental results presented in the paper “GNNPilot: A Holistic Framework for High-Performance Graph Neural Network Computations on GPUs”\cite{1}. The first purpose of this reproduction is to increase the reproducability, preparing a good instructions what needed, help others to reproduce this paper. Secondly, validate the effectiveness of GNNPilot’s proposed optimization strategies, including its neighbor packing and bin packing techniques for sparse operator acceleration, as well as its dynamic parallelization, kernel fusion, and auto-tuning mechanisms. By replicating the key experiments on a different hardware platform, we aim to assess whether GNNPilot achieves comparable performance improvements under less capable computational resources.  At the end of this paper, we believe that GNNPilot will be reproduced much easily and the experiments and results will be validated again. 

\section{Introduction}
Graph Neural Networks (GNNs) have become a milestone of modern machine learning, owing to their ability to model complex topological relationships in data. By capturing dependencies among nodes and edges, GNNs enable efficient learning on graph-structured inputs for tasks such as node classification, link prediction, and graph-level regression. Consequently, they have found broad applicability across domains including social-network analysis, bioinformatics, and knowledge-graph reasoning.

Despite this success, GNNs present unique computational challenges. Unlike convolutional or transformer architectures, GNNs rely heavily on sparse operations—most notably Sparse Matrix--Matrix Multiplication (SpMM) and Sampled Dense--Dense Matrix Multiplication (SDDMM)—that lead to irregular memory access patterns and poor GPU utilization. These irregularities manifest as load imbalance among threads and inefficient cache reuse, resulting in substantial performance degradation when executed on commodity GPUs.

GNNPilot was proposed as a holistic solution to these issues. It introduces two complementary packing strategies—neighbor packing for improved load balancing in sparse graphs and bin packing (with the novel \texttt{BIN\_CSR}
 format) for enhanced data locality in denser graphs. Beyond operator-level improvements, GNNPilot employs dynamic parallelization and a row-panel-based kernel fusion technique to optimize multi-operator workflows. Finally, it integrates a lightweight sampling-based auto-tuning mechanism that adaptively selects near-optimal kernel configurations based on input graph characteristics. Collectively, these contributions enable GNNPilot to achieve significant speedups across diverse GNN models and datasets.

In this reproduction, we aim to validate some of these claims under different hardware conditions. Specifically, we rebuild and execute the open source GNNPilot codebase, evaluate its gather and end-to-end inference performance on representative graph datasets, and analyze the influence of packing strategies and auto-tuning on execution efficiency.

\section{Methodology and Results}
\subsection{Experimental Setup}

\textbf{Platform.}
All experiments were conducted on a single NVIDIA GTX 1060 using the software environment summarized in Table~\ref{tab:platform}.

\begin{table}[h]
\centering
\caption{Hardware and Software Configuration}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
GPU & NVIDIA GTX 1060 6GB\\
CPU & AMD Ryzen 5 5600X (12) @3.700 GHz\\
Memory & 16 GB DDR4 \\
Operating System & Ubuntu 20.04.6 LTS x86_64\\
 Kernel&5.15.0-139-generic\\
CUDA Toolkit & Version 11.4 \\
PyTorch & Version 1.10.2 + cu113 backend \\
MKL & Intel Math Kernel Library (2022.2.1)\\
Compiler & GCC 9.4.0 \\
Python & Version 3.9 \\
\bottomrule
\end{tabular}
\label{tab:platform}
\end{table}

\textbf{Models and Kernels.}
Our reproduction focuses on the single-operator and end-to-end evaluations of the Graph Attention Network (GAT) model, as described in the original paper. The framework executes 32 CUDA kernel variants \texttt{gat\_kernel\_0} to \texttt{gat\_kernel\_31}, representing different combinations of packing and parallelization strategies. Auto-tuning was enabled to select the optimal kernel configuration based on runtime performance.

\textbf{Datasets.}
To evaluate GNNPilot's performance across diverse graph characteristics, we tested 9 datasets from SuiteSparse Matrix Collection and Open Graph Benchmark (OGB): \texttt{polbooks} (105 nodes), \texttt{delaunay\_n10} (1K nodes), \texttt{add20} (2.4K nodes), \texttt{bcsstk13} (2K nodes), \texttt{Erdos971} (472 nodes), \texttt{ca-GrQc} (5.2K nodes), \texttt{email-Enron} (36K nodes), \texttt{caidaRouterLevel} (192K nodes), and \texttt{ogbn-arxiv} (169K nodes). These datasets span sparse to dense graphs with average degrees from 2.78 to 10.02. Dataset statistics are summarized in Table~\ref{tab:datasets}. All datasets were preprocessed and stored locally. Each test was repeated three times, and the average runtime was reported.



\textbf{Metrics.}
We primarily measure per-kernel execution time and end-to-end layer latency, which form the core evaluation metrics in our reproduction. For each kernel variant, we record the minimum runtime over multiple warm-up and measurement iterations. Preprocessing time is also reported to capture the one-time cost of preparing graph structures before kernel execution. Due to hardware limitations and the lack of full access to NVIDIA performance counters on our platform, we were unable to collect detailed profiling metrics such as GPU occupancy, L1/L2 cache hit rates, or GFLOPS. Instead, our analysis focuses on relative kernel performance, speedup across variants, and qualitative trends that align with the behaviors described in the original paper.


\subsection{Single-Operator Evaluation}

Across 9 datasets, auto-tuning selected different optimal kernels based on graph characteristics (Table~\ref{tab:datasets}). Small sparse graphs (polbooks, add20, delaunay\_n10, bcsstk13, ca-GrQc, email-Enron) converged to \texttt{gat\_kernel\_1}, which implements neighbor-packing optimized for load balancing (Section~3.2). Medium-density graphs (Erdos971) favored \texttt{gat\_kernel\_7} with enhanced scatter strategies. Large graphs (ogbn-arxiv, caidaRouterLevel) selected kernels 15 and 0 respectively, employing bin-packing (Section~3.3) for improved data locality.

These kernels process adjacency data using preprocessed structural descriptors (\texttt{kg\_info}) storing row, edge, and neighbor-group metadata. Thread blocks handle balanced graph row subsets, while warps process consecutive non-zero entries to minimize divergence. This ensures vertices with varying degrees receive proportional workloads, reducing idle threads and improving GPU occupancy.

The diversity of optimal kernel selections validates GNNPilot's core claim: no single parallelization strategy dominates across all graph types, necessitating adaptive auto-tuning. Execution times ranged from 0.050~ms (delaunay\_n10) to 3.55~ms (caidaRouterLevel), with kernel-to-kernel speedups reaching 195$\times$ in the worst-to-best comparison, confirming the critical importance of strategy selection.



\begin{table}[h]
\centering
\caption{Multi-Dataset Performance Summary (no reordering)}
\begin{tabular}{lrrrc}
\toprule
\textbf{Dataset} & \textbf{Nodes} & \textbf{AvgDeg} & \textbf{Best Time} & \textbf{Best Kernel} \\
\midrule
polbooks & 105 & 8.4 & 0.063 ms & 1 \\
delaunay\_n10 & 1,024 & 2.98 & 0.050 ms & 1 \\
add20 & 2,395 & 5.5 & 0.064 ms & 1 \\
bcsstk13 & 2,003 & 41.9 & 0.090 ms & 1 \\
Erdos971 & 472 & 2.78 & 0.075 ms & 7 \\
ca-GrQc & 5,242 & 5.53 & 0.122 ms & 1 \\
email-Enron & 36,692 & 10.02 & 0.618 ms & 1 \\
caidaRouterLevel & 192,244 & 6.34 & 3.549 ms & 0 \\
ogbn-arxiv & 169,343 & 7.9 & 3.381 ms & 15 \\
\midrule
\multicolumn{5}{l}{\textit{Overall: 0.050--3.55 ms range, 195$\times$ max speedup via auto-tuning}} \\
\bottomrule
\end{tabular}
\label{tab:datasets}
\end{table}

The best kernel selection varies by graph structure: kernel 1 dominates small sparse graphs, kernel 7 for medium-density, and kernels 0/15 for large graphs, consistent with the original paper's observation that optimal strategies depend on graph characteristics.

\subsection{Strategy Analysis and Packing Effects}

The detailed per-strategy analysis across all 9 datasets, shown in Fig.~\ref{fig:strategy_analysis}, highlights how gather, scatter, and fusion strategies interact. Neighbor-packing–based kernels (kernel 1) consistently outperformed edge-based variants for sparse graphs with low average degree. Bin-packing strategies (kernels 0, 15) showed clear benefits for larger, higher-degree graphs (caidaRouterLevel, ogbn-arxiv), confirming Section 3.3's claims about improved data locality.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{figures/multi_dataset_results/plots/strategy_analysis.png}
\caption{Strategy combination analysis across 9 datasets showing gather, scatter, and fusion effects. Neighbor-packing (kernel 1) dominates sparse graphs, while bin-packing benefits dense graphs.}
\label{fig:strategy_analysis}
\end{figure}

\subsection{End-to-End Performance and Scalability}

The end-to-end inference tests across 9 datasets reproduced the overall acceleration trends of the original paper. Performance scaled predictably with graph size: small graphs ( \textless 1K nodes) executed in 0.05--0.08~ms, medium graphs (1K--10K nodes) in 0.09--0.62~ms, and large graphs (\textgreater 100K nodes) in 3.38--3.55~ms. The consistent speedups from auto-tuning (195$\times$ maximum) demonstrate GNNPilot's effectiveness across this range.

The scalability trends and per-dataset optimization effectiveness are shown in Fig.~\ref{fig:dataset_scalability}. Notably, different graph sizes benefit from different strategies: neighbor packing dominates the sparse regime (polbooks through ca-GrQc), while bin packing becomes advantageous for denser, larger graphs (email-Enron, caidaRouterLevel, ogbn-arxiv).

\begin{figure}[h]
\centering
\includegraphics[width=0.9\columnwidth]{figures/multi_dataset_results/plots/dataset_scalability.png}
\caption{Multi-dataset scalability analysis showing performance across 9 graphs (105 to 192K nodes). Best/average/worst execution times and optimization ratios demonstrate auto-tuning effectiveness. Neighbor packing dominates small sparse graphs, bin packing benefits large dense graphs.}
\label{fig:dataset_scalability}
\end{figure}
\begin{table}[h]
\centering
\caption{Overall Multi-Dataset Reproduction Summary}
\begin{tabular}{lrrr}
\toprule
\textbf{Metric} & \textbf{Best} & \textbf{Mean} & \textbf{Worst} \\
\midrule
Execution Time (ms) & 0.050 & 1.745 & 9.782 \\
Speedup (best/worst) & \multicolumn{3}{c}{195.6$\times$} \\
Datasets Tested & \multicolumn{3}{c}{9} \\
Kernels per Dataset & \multicolumn{3}{c}{32} \\
Total Experiments & \multicolumn{3}{c}{726} \\
Graph Size Range & \multicolumn{3}{c}{105--192K nodes} \\
\bottomrule
\end{tabular}
\label{tab:summary}
\end{table}

\subsection{Successfully Reproduced Components}
In this section, we specify exactly what we have reproduced from the original paper and indicate the corresponding sections as they appear in the original work.

\textbf{Section 3: Gather Operator Optimizations}
\begin{itemize}
    \item All four neighbor packing variants (\texttt{kg\_csr\_balance} through \texttt{kg\_csr\_balance4}) for load balancing in sparse graphs
    \item Complete bin packing implementation with BIN\_CSR sparse format
    \item Auto-tuning mechanism testing $\alpha \in \{1, 5, 10, 15\}$ and adaptive wsize selection
    \item Two-level pointer structure (BinPtr + PckPtr) for improved data locality
\end{itemize}

\textbf{Section 4: Multi-Operator Optimizations}
\begin{itemize}
    \item Dynamic parallelization with four methods: vertex-wise, vertex-edge-wise, edge-wise, and dimension-wise
    \item Row panel-based kernel fusion implementing three strategies: no fusion, node-edge fusion, and all-dimension fusion
    \item Sampling-based auto-tuning across all 32 kernel variants
    \item Hetero+ dynamic parallelization mode with short/long panel differentiation
\end{itemize}

\subsection{Incomplete Components and Limitations}
Despite successful algorithm reproduction, several experimental components could not be fully replicated due to the following reasons:

\textbf{Hardware Constraints:}
\begin{itemize}
    \item Limited to single GPU (GTX 1060 6GB) versus paper's RTX 3080Ti and Tesla A100
    \item Tested 9 diverse datasets (SuiteSparse + OGB) covering 105--192K nodes, versus paper's full 339 SuiteSparse matrices and 8 OGB datasets
    \item Could not test largest datasets (ogbn-products, ogbn-papers100M) due to 6GB memory limit
\end{itemize}

\textbf{Missing External Dependencies:}
\begin{itemize}
    \item Rabbit\cite{3} graph reordering library unavailable—every test was run with preprocessing enabled and with no rabbit reordering in place.
    \item External baseline frameworks (UGCG\cite{2}) not fully integrated for performance comparison
\end{itemize}

\textbf{Experimental Scope Limitations:}
\begin{itemize}
    \item Reproduction was restricted to the GAT model; the GCN, GIN, and GMM models were not evaluated because they serve as baselines in the DGL library, for which we encountered some compatibility issues.
    \item Multi-GPU scaling experiments not conducted
    \item Roofline performance model analysis not performed
\end{itemize}

Table~\ref{tab:coverage} summarizes the reproduction coverage compared to the original paper.

\begin{table}[h]
\centering
\caption{Reproduction Coverage vs. Original Paper}
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{Original} & \textbf{Reproduced} \\
\midrule
\multicolumn{3}{l}{\textit{Section 3: Gather Optimizations}} \\
Neighbor packing (4 variants) & \ding{51} & \ding{51} \\
Bin packing (BIN\_CSR) & \ding{51} & \ding{51} \\
Auto-tuning ($\alpha$, wsize) & \ding{51} & \ding{51} \\
\midrule
\multicolumn{3}{l}{\textit{Section 4: Multi-Operator}} \\
Dynamic parallelization & \ding{51} & \ding{51} \\
Kernel fusion (3 strategies) & \ding{51} & \ding{51} \\
32-kernel auto-tuning & \ding{51} & \ding{51} \\
\end{tabular}
\label{tab:coverage}
\end{table}

\section{Challenges and Solutions}

\subsection{Timeline and Effort Distribution}
The reproduction process spanned 6 weeks (October 15--November 30, 2025) Time distribution:
\begin{itemize}
    \item \textbf{Weeks 1--4}: Understand what Authors did in this paper generally. In the meantime tried to setup the reproduction environment.
    \item \textbf{Weeks 4--6}:  Communicated with Authors about requesting missing parts and getting their environment setup for aligning with the project and making more realistic comparison.
    \item \textbf{Weeks 5--8}: Building the code, generating results, visualize them, writing the paper, iteration.
\end{itemize}

Over 50\% of the time spent understanding what Authors did and setting up the environment. Requirements was relatively old so that we had to switch our own machines to specific setup for that time.
 
\subsection{Initial Obstacles}

\textbf{Codebase Complexity and Missing Components.}
The open-source repository integrated code from multiple prior research papers (PckGNN~\cite{1}, UBG~\cite{2}) without clear modular separation. Critical issues included:
\begin{itemize}
    \item Missing preprocessing kernel implementations (\texttt{preprocessing.cu}) that served as the baseline
    \item Test scripts requiring external libraries (UGCG, Rabbit\cite{3}  reordering) not publicly available (We couldn't find it even it was available)
\end{itemize}

Initial compilation attempts produced over 50 undefined symbol errors. Manual code inspection revealed that core preprocessing functions were entirely absent from the public repository. 

\textbf{Dependency Resolution Challenges.}
The repository provided no dependency specification file (\texttt{requirements.txt}, \texttt{environment.yml}), forcing systematic trial-and-error testing. Through exhaustive version compatibility testing, we created a Github issue on the repository and we managed to create a contact with the Authors and received what we need for environment setup:
\begin{itemize}
    \item PyTorch 1.10.2 specifically
    \item CUDA Toolkit 11.4
    \item Intel oneAPI MKL 2022.2.1 (current version of MKL gave iJKL)
    \item NumPy $<$1.24 (newer versions incompatible with PyTorch 1.10.x C++ extensions)
    \item GCC 9.4.0 specifically
\end{itemize}

We spent over 4 weeks before receiving the correct environment setup and requirements which has ben given by Authors itself.

\textbf{Hardware Memory Constraints.}
The GTX 1060's 6GB VRAM proved insufficient for several experiment configurations:
\begin{itemize}
    \item \texttt{ogbn-products} dataset (2.4M nodes) caused out-of-memory errors during graph preprocessing
    \item \texttt{ogbn-papers100M} crashed immediately upon loading
\end{itemize}

\subsection{Solution Strategies}

\textbf{Author Communication via GitHub Issues.}
After nearly abandoning the reproduction in early November, we created a detailed GitHub issue documenting our reproduction goals and listing specific missing components. The original authors, particularly Zhengding Hu, responded constructively:
\begin{itemize}
    \item Confirmed that \texttt{preprocessing.cu}  and preprocessing.h was accidentally omitted from the public release
    \item Uploaded missing kernel implementations to the repository on November 9, 2025 https://github.com/USTC-ADA/GNNPilot/issues/1#issuecomment-3507584517
    \item Clarified which code components were stable versus research prototypes
    \item Scheduled a Zoom meeting (November 23, 2025) where Mr.~Hu provided a walkthrough of the build process and explained multi-paper code dependencies
\end{itemize}

This collaboration proved essential, without author engagement, reproduction would have been impossible rather than merely difficult.

\textbf{Rabbit Reordering Removal.}
Since the Rabbit graph reordering library was unavailable and unpublished:
\begin{itemize}
    \item Systematically commented out all Rabbit-dependent code paths in test scripts
    \item Modified graph loading routines to use natural (unreordered) adjacency representation
    \item Accepted that absolute L1/L2 cache hit rate comparisons could not be reproduced
    \item Focused evaluation on relative kernel performance rather than absolute cache metrics
\end{itemize}

This compromise allowed reproduction of core algorithmic functionality while acknowledging that cache-related performance claims could not be validated.

\subsection{Lessons for Research Code Release}
This reproduction experience highlighted several best practices for improving research reproducibility:
\begin{enumerate}
    \item \textbf{Complete dependency specification}: Provide exact version requirements in \texttt{requirements.txt} or \texttt{environment.yml}
    \item \textbf{Minimal working example}: Include one-command build and test scripts for core functionality
    \item \textbf{Modular code organization}: Clearly separate baseline implementations from research optimizations
    \item \textbf{Comprehensive documentation}: Explicit README with prerequisites, known limitations, and troubleshooting guides
    \item \textbf{Responsive author engagement}: GitHub issue responsiveness proved crucial to successful reproduction
\end{enumerate}

Without the authors' willingness to provide missing code and clarify build requirements, this reproduction would have been rated ``impossible'' rather than ``difficult but successful.'' 

\section{Conclusion}

In this study, we reproduced the core experimental results of the \textit{GNNPilot} framework on a constrained hardware platform and under realistic software limitations. Despite challenges related to missing dependencies, incomplete documentation, and the substantial effort required to reconstruct the backend and environment, we were able to execute all major kernel variants and end-to-end GAT evaluations. Our reproduction confirms several key findings reported in the original work.

First, in line with Section~3 of the paper, we observed that neighbor packing is highly effective for sparse graphs such as \texttt{ogbn-arxiv}, enabling significant improvements in load balancing and overall kernel runtime. Second, our analysis of parallelization and fusion strategies validates the benefits of GNNPilot’s dynamic method selection (Section~4), with the auto-tuning mechanism consistently identifying top-performing kernels across the 32-variant search space. Third, the end-to-end inference experiments reproduce the observed performance trends: GNNPilot maintains strong speedups even on limited hardware, supporting the claim that its optimizations generalize across different sparsity regimes.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{1}
\bibitem{1} Z. Hu, J. Sun, and G. Sun, ``GNNPilot: A Holistic Framework for High-Performance Graph Neural Network Computations on GPUs,'' \textit{ACM Trans. Arch. Code Optim.}, vol. 22, no. 2, 2025.
\bibitem{2} Hengrui Zhang, Zhongming Yu, Guohao Dai, Guyue Huang, Yufei Ding, Yuan Xie, and Yu Wang. 2022. Understanding gnn computational graph: A coordinated computation, io, and memory perspective. Proceedings of Machine Learning and Systems 4, 1 (2022), 467–484. Retrieved April 29, 2025 from https://proceedings.mlsys.org/paper\_files/paper/2022/ hash/b559156047e50cf316207249d0b5a6c5-Abstract.html

\bibitem{3} Junya Arai, Hiroaki Shiokawa, Takeshi Yamamuro, Makoto Onizuka, and Sotetsu Iwamura. 2016. Rabbit order: Justin-time parallel reordering for fast graph analysis. In Proceedings of the 2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS). IEEE, 22–31. DOI:https://doi.org/10.1109/IPDPS.2016.110
\end{thebibliography}

\end{document}
