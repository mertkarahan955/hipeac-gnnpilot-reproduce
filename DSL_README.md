# DSL Code Generation and Build Guide

This guide explains how to generate CUDA code from DSL (Domain Specific Language) files and build the resulting libraries.

## Overview

The DSL system allows you to define GNN operations in a high-level language and automatically generate optimized CUDA kernels. The workflow is:

1. **Write DSL file** - Define your GNN layer operations
2. **Generate CUDA code** - Run `dsl_run.py` to generate `.cu` and `CMakeLists.txt`
3. **Build library** - Compile the generated code into a shared library
4. **Use in Python** - Load and use the library in your test scripts

## Prerequisites

- Python 3.x
- CUDA toolkit
- PyTorch (with CUDA support)
- CMake (>= 3.14)
- C++ compiler with C++14 support

## Required Files

Before running `dsl_run.py`, ensure you have the preprocessing files:

```
preprocessing_src/
├── preprocessing.h    # Header file with data structures
└── preprocessing.cu   # Implementation of preprocessing functions
```

These files are **static/boilerplate** code and should exist in the repository root. They are not generated by `dsl_run.py` but are required for building.

## Step-by-Step Guide

### 1. Prepare DSL File

Create or use an existing DSL file (e.g., `example_gat_layer.txt`):

```text
f: v_data(fd)
fo: v_data(fd)
we: data(fd, 2)
lr: data(1, 1)
e:  v_data(2)
em: v_data(1)
es: v_data(1)
h:  e_data(1)
ho: e_data(1)
h(nnz, 0) = scatter("e(i, 0) + e(j, 1)")
em(i, 0) = gather(MAX, "max(h(nnz, 0) * lr(0, 0), h(nnz, 0))")
ho(nnz, 0) = scatter("exp(h(nnz, 0) - em(i, 0))")
es(i, 0) = gather(SUM, "ho(nnz, 0)")
fo(i, :) = gather(SUM, "ho(nnz, 0) * f(j, :) / es(i, 0)")
```

### 2. Generate CUDA Code

Run `dsl_run.py` with your DSL file and desired module name:

```bash
python dsl_run.py example_gat_layer.txt gat
```

This will generate:
- `gat.cu` - CUDA kernel code
- `CMakeLists.txt` - CMake build configuration

**Note:** The module name (e.g., `gat`) will be used to create the library `libgat.so` and Python namespace `torch.ops.gatlib`.

### 3. Build the Library

Create a build directory and compile:

```bash
# Create build directory
mkdir -p build
cd build

# Configure CMake (adjust paths as needed)
cmake \
  -DCMAKE_PREFIX_PATH="<path_to_torch>" \
  -DCUDA_ARCH=86 \
  ..

# Build (adjust -j for your CPU cores)
make -j4
```

**Important CMake variables:**
- `CMAKE_PREFIX_PATH`: Path to PyTorch installation (e.g., `~/miniforge/envs/your_env/lib/python3.9/site-packages/torch/`)
- `CUDA_ARCH`: CUDA compute capability (e.g., `86` for A100, `61` for GTX 1060, `75` for T4)

After successful build, you should have:
- `build/libgat.so` (or `lib<module_name>.so`)

### 4. Use in Python

Load and use the library in your Python scripts:

```python
import torch

# Load the generated library
torch.ops.load_library("../build/libgat.so")

# Use preprocessing
info = torch.ops.gatlib.preprocessing(rowptr, indices, 1)

# Call generated kernels
torch.ops.gatlib.gat_kernel_0(info, rowptr, indices, fd, f, fo, we, lr, e, em, es, h, ho)
# ... test other kernel versions (gat_kernel_1, gat_kernel_2, etc.)
```

## File Structure

```
.
├── dsl_run.py                    # Main DSL code generator
├── example_gat_layer.txt         # Example DSL file
├── preprocessing_src/            # Static preprocessing files (REQUIRED)
│   ├── preprocessing.h
│   └── preprocessing.cu
├── backend/                      # DSL parser and generator
│   ├── parser/                   # DSL parsing
│   ├── generator/                # CUDA code generation
│   └── ...
├── build/                        # Build directory (created)
│   ├── CMakeLists.txt           # Generated
│   ├── gat.cu                   # Generated
│   └── libgat.so                # Compiled library
└── test/                         # Test scripts
    └── test_kernel.py
```

## Troubleshooting

### Missing preprocessing files

**Error:** `preprocessing.h: No such file or directory`

**Solution:** Ensure `preprocessing_src/preprocessing.h` and `preprocessing_src/preprocessing.cu` exist in the repository root.

### CMake can't find Torch

**Error:** `Could not find a package configuration file provided by "Torch"`

**Solution:** Set `CMAKE_PREFIX_PATH` to your PyTorch installation:
```bash
cmake -DCMAKE_PREFIX_PATH="$(python -c 'import torch; print(torch.__path__[0])')" ..
```

### CUDA architecture mismatch

**Error:** CUDA compilation errors or "no kernel image is available"

**Solution:** Set `CUDA_ARCH` to match your GPU:
```bash
# Check your GPU compute capability
nvidia-smi --query-gpu=compute_cap --format=csv

# Set in CMake
cmake -DCUDA_ARCH=75 ..  # Example for T4 (sm_75)
```

### Library not found in Python

**Error:** `OSError: libgat.so: cannot open shared object file`

**Solution:** 
1. Ensure the library was built successfully
2. Use absolute path or correct relative path
3. Check library dependencies: `ldd build/libgat.so`

## DSL Syntax Reference

### Data Declarations

- `v_data(fd)` - Vertex (node) data with feature dimension `fd`
- `e_data(fd)` - Edge data with feature dimension `fd`
- `data(dim1, dim2)` - Constant data with dimensions

### Operations

- `scatter("expression")` - Scatter operation (edge to node)
- `gather(OP, "expression")` - Gather operation (node to node/edge)
  - `OP` can be `SUM`, `MAX`, `AVG`
- `linear(A, B)` - Matrix multiplication

### Indexing

- `i` - Current node index
- `j` - Neighbor node index
- `nnz` - Edge index
- `:` - All dimensions

## Example: Complete Workflow

```bash
# 1. Generate code
python dsl_run.py example_gat_layer.txt gat

# 2. Build
mkdir -p build && cd build
cmake -DCMAKE_PREFIX_PATH="$(python -c 'import torch; print(torch.__path__[0])')" -DCUDA_ARCH=86 ..
make -j4

# 3. Test (from test directory)
cd ../test
python test_kernel.py ../bcsstk13.mtx
```

## Notes

- The generated code creates multiple kernel versions (e.g., `gat_kernel_0` through `gat_kernel_31`) with different optimization strategies
- Preprocessing files (`preprocessing_src/`) are **static** and shared across all DSL builds
- Each DSL build creates a separate library with its own namespace
- The `long_dynamic` parameter in preprocessing is currently unused but reserved for future optimizations

